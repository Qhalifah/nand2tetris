import JackTokenizer
import unittest

class JackTokenizerTest(unittest.TestCase):
    def test_is_identifier(self):
        tokenizer = JackTokenizer.JackTokenizer("test.jack")
        self.assertFalse(tokenizer.is_identifier('123abc'))
        self.assertFalse(tokenizer.is_identifier('abc 123'))
        self.assertTrue(tokenizer.is_identifier('abc123'))
        self.assertTrue(tokenizer.is_identifier('_abc123'))

    def test_tokenizer(self):
        tokenizer = JackTokenizer.JackTokenizer("test.jack")
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, 'class')
        self.assertEqual(tokenizer.token_type(), 'keyword')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, 'A')
        self.assertEqual(tokenizer.token_type(), 'identifier')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, '{')
        self.assertEqual(tokenizer.token_type(), 'symbol')

        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, 'let')
        self.assertEqual(tokenizer.token_type(), 'keyword')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, 'x')
        self.assertEqual(tokenizer.token_type(), 'identifier')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, '=')
        self.assertEqual(tokenizer.token_type(), 'symbol')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, '-')
        self.assertEqual(tokenizer.token_type(), 'symbol')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, '4')
        self.assertEqual(tokenizer.token_type(), 'integer_constant')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, ';')
        self.assertEqual(tokenizer.token_type(), 'symbol')

        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, 'do')
        self.assertEqual(tokenizer.token_type(), 'keyword')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, 'Output')
        self.assertEqual(tokenizer.token_type(), 'identifier')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, '.')
        self.assertEqual(tokenizer.token_type(), 'symbol')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, 'printString')
        self.assertEqual(tokenizer.token_type(), 'identifier')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, '(')
        self.assertEqual(tokenizer.token_type(), 'symbol')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, '"Huzzah!"')
        self.assertEqual(tokenizer.token_type(), 'string_constant')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, ')')
        self.assertEqual(tokenizer.token_type(), 'symbol')
        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, ';')
        self.assertEqual(tokenizer.token_type(), 'symbol')

        tokenizer.has_more_tokens()
        tokenizer.advance()
        self.assertEqual(tokenizer.current_token, '}')
        self.assertEqual(tokenizer.token_type(), 'symbol')
